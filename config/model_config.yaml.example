# 模型配置

# === 模型提供者設定 ===
# provider.type 選項：
#   - local: 本機載入 Transformers 模型（需要 GPU/CPU 資源）
#   - openai: OpenAI 相容 API（支援 vLLM、Ollama、OpenAI、LM Studio 等）
#   - huggingface: Hugging Face Inference Endpoint
provider:
  type: "local"  # 目前使用本機載入

  # Local Provider 設定
  local:
    name: "TAIDE-LX-7B-Chat"
    path: "models/TAIDE-LX-7B-Chat"
    force_cpu: false  # 使用 GPU + 4-bit 量化
    max_gpu_memory: 8
    quantization:
      enable_4bit: true  # 啟用 4-bit 量化（Windows 相容版 bitsandbytes 已安裝）
      load_in_4bit: true
      offload:
        device_map: "auto"
        offload_folder: "./offload"

  # # === OpenAI 相容 API 設定（當 type=openai 時使用）===
  # # 使用方式：先部署推論服務（vLLM/Ollama/LM Studio），再設定以下參數
  # openai:
  #   # API Base URL
  #   # - vLLM: http://localhost:8000/v1
  #   # - Ollama: http://localhost:11434/v1
  #   # - OpenAI: https://api.openai.com/v1
  #   api_base: "http://localhost:8000/v1"
  #   # API Key（選填，某些本地部署不需要）
  #   api_key: null
  #   # 模型名稱（須與 API 端點的模型名稱對應）
  #   model: "taide/TAIDE-LX-7B"
  #   # 請求逾時（秒）
  #   timeout: 120
  #   # 最大重試次數
  #   max_retries: 2

  # # === Hugging Face Inference Endpoint 設定（當 type=huggingface 時使用）===
  # # 使用方式：在 HF 建立 Inference Endpoint，取得 URL 與 Token
  # huggingface:
  #   # Inference Endpoint URL（從 HF 控制台取得）
  #   endpoint_url: "https://your-endpoint.endpoints.huggingface.cloud"
  #   # Hugging Face API Token（必填，從 HF Settings → Access Tokens 取得）
  #   api_token: "hf_your_token_here"
  #   # 請求逾時（秒）
  #   timeout: 120
  #   # 最大重試次數
  #   max_retries: 2

# === 模型清單 / 切換設定（002-model-switch-container）===
# 注意：此段為新功能預留設定鍵；目前程式若未使用，會被安全地忽略，不影響既有行為。
models:
  # 模型目錄掃描（用於列出可用模型清單）
  catalog:
    enabled: true
    # directory: "models"  # 預設使用 Django settings.MODELS_DIR；需要時才覆寫
    required_config: "config.json"  # 最低門檻：models/<model_id>/config.json 存在

  # 預設模型（當使用者尚未選擇或選擇失效時的 fallback）
  selection:
    default_model_id: "TAIDE-LX-7B-Chat"

  # 模型切換政策
  switching:
    # policy:
    #   - explicit: 需呼叫切換 API 才會切換
    #   - lazy: 翻譯請求帶 model_id 時可自動切換（預計於 US1 實作）
    policy: "explicit"
    lock_timeout_seconds: 30

# 生成參數設定
generation:
  # 快速模式參數
  fast:
    temperature: 0.4   # 降低以提升穩定性（原 0.7）
    top_p: 0.85        # 降低以減少隨機性（原 0.9）
    num_beams: 1
    do_sample: true
    min_new_tokens: 5  # 提高到 5，避免提早結束（原 1）
    max_new_tokens: 256
    max_tokens: 256
    repetition_penalty: 1.2  # 降低避免過度懲罰（原 1.5）
    no_repeat_ngram_size: 3

  # 標準模式參數
  standard:
    temperature: 0.3  # 降低以提升穩定性（原 0.5）
    top_p: 0.85
    num_beams: 1
    do_sample: true
    min_new_tokens: 10  # 提高到 10，確保生成足夠內容（原 5）
    max_new_tokens: 512
    max_tokens: 512
    repetition_penalty: 1.2  # 降低避免過度懲罰（原 1.5）
    no_repeat_ngram_size: 3

  # 高品質模式參數
  high:
    temperature: 0.2
    top_p: 0.8
    num_beams: 4
    do_sample: false
    min_new_tokens: 10  # 提高到 10，確保生成品質（原 1）
    max_new_tokens: 1024
    max_tokens: 1024
    repetition_penalty: 1.2  # 降低避免過度懲罰（原 1.5）
    no_repeat_ngram_size: 3

# Prompt 範本
prompts:
  # === Prompt 格式設定 ===
  # format_type 選項：
  #   - template: 使用手動組裝的 prompt 樣版（如 [INST] ... [/INST]）
  #   - chat_template: 使用 Huggingface tokenizer.apply_chat_template()
  format_type: "chat_template"

  # === BOS Token 設定 ===
  # 是否在 prompt 前添加 <s> (BOS token)
  # - Chat 模型通常需要（如 TAIDE-LX-7B-Chat）
  # - 基礎預訓練模型可能不需要（如 TAIDE-LX-7B）
  # 注意：使用 chat_template 時，tokenizer 會自動處理，此設定僅對 template 模式生效
  add_bos_token: true

  # === System Prompt 設定 ===
  # 是否使用 system prompt（<<SYS>> ... <</SYS>>）
  # 某些模型支援 system prompt 來設定角色和行為
  use_system_prompt: true
  system_prompt: "你是專業翻譯員。你的任務是『翻譯』，不是改寫、續寫或創作。"

  # === 翻譯 Prompt 範本 ===
  # 當 format_type=template 時使用
  # 格式說明：
  #   - 不含 <s>: [INST] 指令 [/INST]（適用於 tokenizer 自動添加 BOS 的情況）
  #   - 含 <s>: <s>[INST] 指令 [/INST]（手動添加 BOS）
  #   - 含 system: <s>[INST] <<SYS>>\n{sys}\n<</SYS>>\n\n{指令} [/INST]
  # 可用變數：{source_language}, {target_language}, {text}
  # 使用 |- 移除尾部換行符
  translation: |-
    [INST] 你是專業翻譯員。你的任務是『翻譯』，不是改寫、續寫或創作。
    請將下列文字從 {source_language} 翻譯成 {target_language}。

    要求：
    - 只輸出譯文本身，不要輸出「譯文：」等前綴
    - 不要輸出任何解釋、補充說明或延伸內容
    - 不要產生章節、標題、目錄或無關文字
    - 保持原文的行數和結構，不要增加或刪減內容

    原文：
    {text} [/INST]

  # === Chat Template 用的訊息內容 ===
  # 當 format_type=chat_template 時使用
  # 這是給 tokenizer.apply_chat_template() 的 user message 內容
  translation_chat_content: |-
    你是專業翻譯員。你的任務是『翻譯』，不是改寫、續寫或創作。
    請將下列文字從 {source_language} 翻譯成 {target_language}。

    要求：
    - 只輸出譯文本身，不要輸出「譯文：」等前綴
    - 不要輸出任何解釋、補充說明或延伸內容
    - 不要產生章節、標題、目錄或無關文字
    - 保持原文的行數和結構，不要增加或刪減內容

    原文：
    {text}

  # === 語言偵測 Prompt 範本 ===
  language_detection: |-
    [INST] 請識別以下文字的語言，只回答語言代碼（zh-TW, zh-CN, en, ja, ko, fr, de, es 其中之一）和信心分數（0.0-1.0），格式為「語言代碼:信心分數」。

    文字：{text} [/INST]

  # 語言偵測的 chat template 內容
  language_detection_chat_content: |-
    請識別以下文字的語言，只回答語言代碼（zh-TW, zh-CN, en, ja, ko, fr, de, es 其中之一）和信心分數（0.0-1.0），格式為「語言代碼:信心分數」。

    文字：{text}
