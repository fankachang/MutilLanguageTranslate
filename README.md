# å¤šåœ‹èªè¨€ç¿»è­¯ç³»çµ±

åŸºæ–¼ TAIDE-LX-7B å¤§å‹èªè¨€æ¨¡å‹çš„å¤šåœ‹èªè¨€ç¿»è­¯ç³»çµ±ï¼Œæ¡ç”¨ Django ASGI + HTMX + Alpine.js æŠ€è¡“æ¶æ§‹ï¼Œæä¾›å³æ™‚ç¿»è­¯ã€æ­·å²è¨˜éŒ„ã€ä½¿ç”¨è€…è¨­å®šç­‰åŠŸèƒ½ã€‚

## åŠŸèƒ½ç‰¹é»

- ğŸŒ **å¤šèªè¨€æ”¯æ´**ï¼šæ”¯æ´ç¹é«”ä¸­æ–‡ã€ç°¡é«”ä¸­æ–‡ã€è‹±æ–‡ã€æ—¥æ–‡ã€éŸ“æ–‡ã€è¶Šå—æ–‡ã€æ³°æ–‡ã€å°å°¼æ–‡ç­‰ 8 ç¨®èªè¨€
- ğŸ”„ **è‡ªå‹•èªè¨€åµæ¸¬**ï¼šæ™ºæ…§åµæ¸¬è¼¸å…¥æ–‡å­—çš„ä¾†æºèªè¨€
- âš¡ **å³æ™‚ç¿»è­¯**ï¼šä½å»¶é²ç¿»è­¯å›æ‡‰ï¼Œæ”¯æ´ GPU åŠ é€Ÿ
- ğŸ“ **ç¿»è­¯æ­·å²**ï¼šç€è¦½å™¨æœ¬åœ°å„²å­˜ï¼Œä¿ç•™æœ€è¿‘ 20 ç­†ç¿»è­¯è¨˜éŒ„
- âš™ï¸ **å€‹äººåŒ–è¨­å®š**ï¼šæ”¯æ´æ·±è‰²/æ·ºè‰²ä¸»é¡Œã€å­—é«”å¤§å°èª¿æ•´
- ğŸ“Š **ç³»çµ±ç›£æ§**ï¼šå³æ™‚æŸ¥çœ‹ç³»çµ±è³‡æºä½¿ç”¨ç‹€æ³å’Œç¿»è­¯çµ±è¨ˆ

## ç³»çµ±éœ€æ±‚

### ç¡¬é«”éœ€æ±‚

| é …ç›®   | æœ€ä½éœ€æ±‚ | å»ºè­°é…ç½®                |
| ------ | -------- | ----------------------- |
| CPU    | 4 æ ¸å¿ƒ   | 8 æ ¸å¿ƒä»¥ä¸Š              |
| è¨˜æ†¶é«” | 16 GB    | 32 GB ä»¥ä¸Š              |
| GPU    | -        | NVIDIA GPU (16GB+ VRAM) |
| ç£ç¢Ÿ   | 50 GB    | 100 GB SSD              |

### è»Ÿé«”éœ€æ±‚

- Python 3.11ï¼ˆæ¨è–¦ï¼›Windows ä¸Šå‹™å¿…ä½¿ç”¨ 3.11 ä»¥ç¢ºä¿ PyTorch cu118 ç›¸å®¹æ€§ï¼Œä½†è‹¥ç³»çµ±æœ‰ 3.10 ä¹Ÿå¯ä»¥ï¼‰
- CUDA 11.8+ï¼ˆGPU åŠ é€Ÿæ™‚éœ€è¦ï¼‰
- Git

## å¿«é€Ÿé–‹å§‹

### 1. è¤‡è£½å°ˆæ¡ˆ

```bash
git clone <repository-url>
cd MutilLanguageTranslate
```

### 2. å»ºç«‹è™›æ“¬ç’°å¢ƒ

```bash
# è‹¥å°ˆæ¡ˆæ ¹ç›®éŒ„å·²å­˜åœ¨ .venv/ï¼Œè«‹ä¸è¦é‡å»ºï¼Œç›´æ¥å•Ÿç”¨å³å¯ã€‚

python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/macOS
source .venv/bin/activate
```

### 3. å®‰è£ç›¸ä¾å¥—ä»¶

#### CPU æ¨¡å¼ï¼ˆæˆ–å·²å®‰è£ PyTorchï¼‰

```bash
pip install -r requirements.txt
```

#### GPU æ¨¡å¼ï¼ˆNVIDIA CUDAï¼‰

**é‡è¦ï¼š** è«‹ä½¿ç”¨ Python 3.10 æˆ– 3.11ï¼ˆPyTorch cu118 wheel æ”¯æ´é€™äº›ç‰ˆæœ¬ï¼‰ã€‚

```powershell
# å‡ç´š pip
pip install --upgrade pip

# 1. å®‰è£ PyTorch with CUDA 11.8
pip install --index-url https://download.pytorch.org/whl/cu118 torch torchvision

# 2. å®‰è£å°ˆæ¡ˆç›¸ä¾å¥—ä»¶
pip install -r requirements.txt

# 3. (å¯é¸) å®‰è£ bitsandbytes ä»¥å•Ÿç”¨ 4-bit é‡åŒ–ï¼ˆé©ç”¨æ–¼ 8GB VRAM ä»¥ä¸‹çš„ GPUï¼‰
# Windows ä¸Šå¯èƒ½éœ€è¦ Visual Studio Build Tools
pip install bitsandbytes
```

**ç–‘é›£æ’è§£ï¼š**
- è‹¥ Python ç‰ˆæœ¬ç‚º 3.14 ç­‰è¼ƒæ–°ç‰ˆæœ¬ï¼ŒPyTorch å®˜æ–¹ wheel å¯èƒ½å°šæœªæ”¯æ´ï¼Œè«‹ä½¿ç”¨ `py -3.10` æˆ– `py -3.11` å»ºç«‹è™›æ“¬ç’°å¢ƒã€‚
- bitsandbytes åœ¨ Windows ä¸Šå¯èƒ½éœ€è¦ç·¨è­¯å·¥å…·ï¼Œè‹¥å®‰è£å¤±æ•—å¯è·³éï¼ˆç³»çµ±æœƒè‡ªå‹•ä½¿ç”¨ float16 æ¨¡å¼ï¼‰ã€‚

### 4. è¨­å®šé…ç½®æª”

```bash
# è¤‡è£½ç¯„ä¾‹é…ç½®æª”

# Linux/macOS/Git Bash
cp config/app_config.yaml.example config/app_config.yaml
cp config/model_config.yaml.example config/model_config.yaml

# Windows PowerShell
Copy-Item config/app_config.yaml.example config/app_config.yaml
Copy-Item config/model_config.yaml.example config/model_config.yaml

# ä¾æ“šéœ€æ±‚ç·¨è¼¯é…ç½®æª”
```

### 5. ä¸‹è¼‰æ¨¡å‹

æœ¬å°ˆæ¡ˆé è¨­ä½¿ç”¨ **æœ¬æ©Ÿæ¨¡å‹**ï¼ˆ`provider.type: local`ï¼‰ï¼Œä¸”é è¨­è·¯å¾‘ç‚º `models/TAIDE-LX-7B-Chat`ï¼ˆè¦‹ `config/model_config.yaml`ï¼‰ã€‚

- è‹¥ `models/` ç›®éŒ„å·²åŒ…å«æ¨¡å‹è³‡æ–™å¤¾ï¼ˆä¾‹å¦‚ `models/TAIDE-LX-7B-Chat/`ã€`models/Llama-3.1-TAIDE-LX-8B-Chat/`ï¼‰ï¼Œå¯ç›´æ¥å•Ÿå‹•æœå‹™ã€‚
- è‹¥ `models/` å°šæœªæ”¾ç½®æ¨¡å‹ï¼Œè«‹å…ˆæŠŠæ¨¡å‹ä¸‹è¼‰/æ”¾åˆ°å°æ‡‰è·¯å¾‘ï¼ˆå¿…é ˆåŒ…å« `config.json` ç­‰æª”æ¡ˆï¼‰ï¼Œå¦å‰‡è¼‰å…¥æ™‚æœƒå› ã€Œæ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨ã€è€Œå¤±æ•—ã€‚

å‚™è¨»ï¼šREADME å…ˆå‰æåˆ°ã€Œé¦–æ¬¡å•Ÿå‹•è‡ªå‹•ä¸‹è¼‰ã€ä¸ç¬¦åˆç›®å‰é è¨­è¨­å®šï¼ˆç›®å‰é è¨­æ˜¯æƒæ/è¼‰å…¥æœ¬æ©Ÿç›®éŒ„ï¼‰ã€‚å¦‚éœ€æ”¹æˆé ç«¯/è‡ªå‹•ä¸‹è¼‰æ¨¡å¼ï¼Œå»ºè­°æ”¹ç”¨ `provider.type: openai`ï¼ˆæˆ–è‡ªè¡Œèª¿æ•´ç‚º Hugging Face ä¾†æºï¼‰ã€‚

### 6. å•Ÿå‹•æœå‹™

```bash
cd translation_project

# é–‹ç™¼æ¨¡å¼
..\.venv\Scripts\python.exe manage.py runserver

# ç”Ÿç”¢æ¨¡å¼ï¼ˆä½¿ç”¨ uvicornï¼‰
uvicorn translation_project.asgi:application --host 0.0.0.0 --port 8000
```

### 7. é–‹å•Ÿç€è¦½å™¨

é€ è¨ª http://localhost:8000 é–‹å§‹ä½¿ç”¨ç¿»è­¯æœå‹™ã€‚

### é‡è¦ï¼šæ¨¡å‹è¼‰å…¥æ”¹ç‚ºæ‰‹å‹•å•Ÿå‹•

ç‚ºäº†é¿å…å•Ÿå‹•æ™‚å°±ä½”ç”¨å¤§é‡è³‡æºï¼Œæœå‹™å•Ÿå‹•å¾Œ**ä¸æœƒè‡ªå‹•è¼‰å…¥æ¨¡å‹**ã€‚

- è«‹é–‹å•Ÿ http://localhost:8000/admin/status/
- åœ¨ã€Œè¦å•Ÿå‹•çš„æ¨¡å‹ã€é¸æ“‡æ¨¡å‹å¾ŒæŒ‰ä¸‹ã€Œé¸æ“‡å¾Œé–‹å§‹è¼‰å…¥ã€

è‹¥ä½ æƒ³ç¶­æŒèˆŠè¡Œç‚ºï¼ˆå•Ÿå‹•å°±è‡ªå‹•è¼‰å…¥ï¼‰ï¼Œå¯è¨­å®šç’°å¢ƒè®Šæ•¸ï¼š

```powershell
$env:TRANSLATOR_AUTO_LOAD_MODEL_ON_STARTUP = "1"
```

## GPU è¨˜æ†¶é«”æœ€ä½³åŒ–ï¼ˆè‡ªå‹•åµæ¸¬ï¼‰

ç³»çµ±æœƒ**è‡ªå‹•åµæ¸¬** GPU è¨˜æ†¶é«”å¤§å°ä¸¦é¸æ“‡æœ€ä½³è¼‰å…¥æ¨¡å¼ï¼š

- **VRAM â‰¤ 12GB**ï¼ˆä¾‹å¦‚ RTX 4060 8GB, RTX 3060 12GBï¼‰ï¼šè‡ªå‹•å•Ÿç”¨ **4-bit é‡åŒ–**ï¼Œç¯€çœ 70-75% è¨˜æ†¶é«”
- **VRAM > 12GB**ï¼ˆä¾‹å¦‚ RTX 4090 24GBï¼‰ï¼šä½¿ç”¨ **float16 æ¨¡å¼**ï¼Œä¿æŒæœ€ä½³å“è³ª

ç„¡éœ€æ‰‹å‹•è¨­å®šï¼Œç³»çµ±æœƒè‡ªå‹•é¸æ“‡æœ€é©åˆçš„æ¨¡å¼ã€‚

### æ‰‹å‹•è¦–ç›–è‡ªå‹•åµæ¸¬ï¼ˆå¯é¸ï¼‰

è‹¥æ‚¨æƒ³å¼·åˆ¶ä½¿ç”¨ç‰¹å®šæ¨¡å¼ï¼Œå¯ä¿®æ”¹ `config/model_config.yaml`ï¼š

```yaml
# å¼·åˆ¶å•Ÿç”¨ 4-bit é‡åŒ–ï¼ˆå¿½ç•¥è‡ªå‹•åµæ¸¬ï¼‰
quantization:
  enable_4bit: true
  load_in_4bit: true

# æˆ–å¼·åˆ¶åœç”¨ 4-bit é‡åŒ–ï¼ˆä½¿ç”¨ float16ï¼‰
quantization:
  enable_4bit: false
```

### bitsandbytes å®‰è£èªªæ˜

4-bit é‡åŒ–éœ€è¦ `bitsandbytes` å¥—ä»¶ï¼ˆå·²åŒ…å«åœ¨ `requirements.txt` ä¸­ï¼‰ï¼š

- **Linux**: ç›´æ¥å®‰è£å³å¯
- **Windows**: å¯èƒ½éœ€è¦ [Visual Studio Build Tools](https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2022)ï¼ˆC++ é–‹ç™¼å·¥å…·ï¼‰

è‹¥ bitsandbytes å®‰è£å¤±æ•—æˆ–ä¸å¯ç”¨ï¼Œç³»çµ±æœƒè‡ªå‹•é€€å›ä½¿ç”¨ float16 æ¨¡å¼ã€‚

### æ–¹æ³• 2ï¼šCPU/GPU Offloadï¼ˆaccelerateï¼‰

ä½¿ç”¨ `accelerate` å°‡æ¨¡å‹éƒ¨åˆ†å±¤ç§»è‡³ CPU æˆ–ç£ç¢Ÿï¼Œä»¥é©é… GPU è¨˜æ†¶é«”é™åˆ¶ã€‚

```powershell
# accelerate å·²åœ¨ requirements.txt ä¸­ï¼Œç¢ºèªå·²å®‰è£
pip install accelerate
```

`config/model_config.yaml` ä¸­è¨­å®š `max_gpu_memory` ç‚ºæ‚¨çš„ VRAM å¤§å°ï¼š

```yaml
model:
  max_gpu_memory: 8  # 8GB VRAM
```

ç³»çµ±æœƒè‡ªå‹•ä½¿ç”¨ `device_map="auto"` å°‡æ¨¡å‹å±¤åˆ†é…åˆ° GPU/CPU/ç£ç¢Ÿã€‚

### æ¸¬è©¦è¨­å®š

æ‚¨å¯ä»¥ä½¿ç”¨å…§å»ºçš„æ¸¬è©¦ API é©—è­‰è¨­å®šæ˜¯å¦æ­£ç¢ºï¼š

```powershell
# æ¸¬è©¦è¼‰å…¥å°æ¨¡å‹ï¼ˆgpt2ï¼‰ä»¥é©—è­‰ GPU èˆ‡é‡åŒ–è¨­å®š
curl -X POST http://localhost:8000/api/v1/admin/model/test/ -H "Content-Type: application/json" -d '{"model_name": "gpt2"}'
```

æˆ–ä½¿ç”¨æˆ‘å€‘æä¾›çš„æ¸¬è©¦è…³æœ¬ï¼š

```powershell
.venv\Scripts\python tests\quick_model_test.py
```

## å®¹å™¨éƒ¨ç½²ï¼ˆPodman / Docker / Composeï¼‰

> é©—è­‰é‡é»ï¼šæœå‹™å•Ÿå‹•å¾Œ `GET /api/health/` éœ€å› 200ã€‚

### Windows ä½¿ç”¨ Podman çš„å‰ç½®ï¼ˆå¿…è¦ï¼‰

Podman åœ¨ Windows ä¸Šéœ€è¦é€é VMï¼ˆé è¨­ç‚º WSLï¼‰åŸ·è¡Œ Linux containersï¼Œå› æ­¤ç¬¬ä¸€æ¬¡ä½¿ç”¨å‰è«‹å…ˆæŠŠ Podman çš„ machine å•Ÿèµ·ä¾†ï¼š

```powershell
# ç¬¬ä¸€æ¬¡ä½¿ç”¨ï¼ˆæœƒå»ºç«‹ VMï¼‰
podman machine init --now

# ä¹‹å¾Œè¦å•Ÿå‹•/åœæ­¢ VM
podman machine start
podman machine stop

# æª¢æŸ¥ç‹€æ…‹
podman machine list
podman info
```

å»ºè­°å®‰è£ Podman Desktop ä¾†ç®¡ç† machine/æ˜ åƒ/å®¹å™¨ï¼ˆå¯é¸ï¼Œä½†å° Windows æ–¹ä¾¿ï¼‰ã€‚

### ä½¿ç”¨ Podman

```bash
# å»ºç½®æ˜ åƒ
podman build -t translation-service -f Containerfile .

# åŸ·è¡Œå®¹å™¨
podman run -d \
  --name translation-service \
  -p 8000:8000 \
  -v ./models:/app/models:ro \
  -v ./config:/app/config:ro \
  -v ./logs:/app/logs \
  translation-service
```

### ä½¿ç”¨ Podman Composeï¼ˆæ¨è–¦ï¼Œç›´æ¥æ²¿ç”¨ docker-compose.yamlï¼‰

æœ¬å°ˆæ¡ˆå·²æä¾› `docker-compose.yaml`ï¼ŒPodman å¯ä»¥ç”¨ `podman compose` ç›´æ¥å•Ÿå‹•ï¼š

```bash
podman compose -f docker-compose.yaml up -d
```

åœæ­¢ä¸¦æ¸…æ‰å®¹å™¨/ç¶²è·¯ï¼š

```bash
podman compose -f docker-compose.yaml down
```

æ³¨æ„ï¼š`podman compose` æœƒå‘¼å«å¤–éƒ¨ compose providerï¼ˆä¾‹å¦‚ `docker-compose` æˆ– `podman-compose`ï¼‰ã€‚
è‹¥ä½ åŸ·è¡Œ `podman compose` æ™‚æç¤ºæ‰¾ä¸åˆ° providerï¼Œè«‹å…ˆå®‰è£å…¶ä¸­ä¸€å€‹ï¼š

```powershell
# å»ºè­°ï¼šå®‰è£åˆ°æœ¬å°ˆæ¡ˆçš„ .venvï¼Œé¿å…ä½¿ç”¨ç³»çµ± Python
.\.venv\Scripts\python.exe -m pip install podman-compose

# (å¯é¸ï¼Œä½†æ¨è–¦) æŒ‡å®š podman compose è¦ä½¿ç”¨ .venv è£¡çš„ provider
$env:PODMAN_COMPOSE_PROVIDER = (Resolve-Path .\.venv\Scripts\podman-compose.exe).Path
```

æˆ–æ˜¯ç›´æ¥ç”¨ `.venv` å…§çš„ `podman-compose`ï¼ˆå®Œå…¨ä¸ä¾è³´ç³»çµ± Pythonï¼‰ï¼š

```powershell
.\.venv\Scripts\podman-compose.exe -f docker-compose.yaml up -d
```

### ä½¿ç”¨ Docker

```bash
docker build -t translation-service -f Containerfile .

docker run -d \
  --name translation-service \
  -p 8000:8000 \
  -v ./models:/app/models:ro \
  -v ./config:/app/config:ro \
  -v ./logs:/app/logs \
  translation-service
```

### ä½¿ç”¨ Docker Compose

```bash
docker compose -f docker-compose.yaml up -d
```

#### GPU æ³¨æ„äº‹é …ï¼ˆDocker / Podmanï¼‰

- `docker-compose.yaml` ç›®å‰ä»¥ **CDI**ï¼ˆ`nvidia.com/gpu=all`ï¼‰çš„æ–¹å¼å®£å‘Š GPUã€‚
- è‹¥ä½ ä½¿ç”¨ Dockerï¼Œéœ€å…ˆå®‰è£ä¸¦æ­£ç¢ºè¨­å®š NVIDIA Container Toolkitï¼ˆä¸¦å•Ÿç”¨ CDI æˆ–ä½¿ç”¨ç­‰æ•ˆçš„ GPU æ›è¼‰æ–¹å¼ï¼‰ï¼Œå¦å‰‡å®¹å™¨é›–å¯å•Ÿå‹•ä½†ç„¡æ³•ä½¿ç”¨ GPUã€‚
- è‹¥ä½ ä½¿ç”¨ Podman Desktop / Podman machineï¼Œè«‹ç¢ºèª VM å…·å‚™ GPU passthrough èˆ‡å°æ‡‰é©…å‹•æ”¯æ´ã€‚

##### NVIDIA Container Toolkitï¼ˆLinuxï¼‰å¿«é€Ÿå®‰è£/é©—è­‰ï¼ˆå»ºè­°ï¼‰

> ä¸åŒç™¼è¡Œç‰ˆ/ç‰ˆæœ¬å®‰è£æ–¹å¼ç•¥æœ‰å·®ç•°ï¼›ä»¥ä¸‹ä»¥å¸¸è¦‹ Linux ç™¼è¡Œç‰ˆç‚ºä¾‹ã€‚è‹¥ä½ ä¸æ˜¯åœ¨ Linux ä¸Šè·‘å®¹å™¨ï¼ˆä¾‹å¦‚ Windows çš„ Docker Desktop / Podman machineï¼‰ï¼Œè«‹ä»¥è©²å¹³å°çš„ GPU/WSL2 æŒ‡å—ç‚ºæº–ã€‚

1) **å…ˆç¢ºèªä¸»æ©Ÿç«¯ NVIDIA é©…å‹•å¯ç”¨**

```bash
nvidia-smi
```

2) **å®‰è£ NVIDIA Container Toolkit**ï¼ˆåƒè€ƒå®˜æ–¹æ–‡ä»¶ï¼‰

- https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html

3) **å•Ÿç”¨/ç”Ÿæˆ CDI è¨­å®š**ï¼ˆè®“ `nvidia.com/gpu=all` å¯ç”¨ï¼‰

```bash
sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
```

4) **é‡å•Ÿå®¹å™¨å¼•æ“**

- Dockerï¼š`sudo systemctl restart docker`
- Podmanï¼šè‹¥ä½¿ç”¨ `podman`ï¼ˆrootless/rootfulï¼‰èˆ‡ç’°å¢ƒä¸åŒï¼Œè«‹ä¾ä½ çš„æœå‹™ç®¡ç†æ–¹å¼é‡å•Ÿå°æ‡‰ daemon / machine

5) **é©—è­‰ GPU å®¹å™¨å¯ç”¨**

```bash
docker run --rm --gpus all nvidia/cuda:12.3.2-base-ubuntu22.04 nvidia-smi
```

##### Podmanï¼ˆLinuxï¼‰å®Œæ•´æ­¥é©Ÿï¼šå®‰è£ NVIDIA Container Toolkit + CDI

> é€™ä»½æµç¨‹é©ç”¨æ–¼ã€ŒPodman ç›´æ¥è·‘åœ¨ Linux ä¸»æ©Ÿã€çš„æƒ…å¢ƒï¼ˆä¸æ˜¯ Windows çš„ podman machineï¼‰ã€‚

0) **ç¢ºèªä¸»æ©Ÿå·²å®‰è£ NVIDIA é©…å‹•**

```bash
nvidia-smi
```

1) **å®‰è£ nvidia-container-toolkit**

- **Ubuntu / Debian**ï¼ˆå»ºè­°ä¾å®˜æ–¹æ–‡ä»¶æ“ä½œï¼Œæœƒè‡ªå‹•åŠ  repo èˆ‡ keyï¼‰

  - å®˜æ–¹å®‰è£æŒ‡å—ï¼š
    - https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html

- **Fedora / RHEL / Rocky / Alma**

  - åŒä¸Šå®˜æ–¹å®‰è£æŒ‡å—ï¼ˆä¾ä½ çš„ç™¼è¡Œç‰ˆé¸æ“‡å°æ‡‰ç« ç¯€ï¼‰

2) **ç”Ÿæˆ CDI è¨­å®šï¼ˆè®“ Podman å¯ç”¨ `--device nvidia.com/gpu=all`ï¼‰**

```bash
sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
```

3) **é‡æ–°å•Ÿå‹• / é‡æ–°é€²å…¥ Podman sessionï¼ˆè¦–ç’°å¢ƒè€Œå®šï¼‰**

```bash
podman info | head
```

4) **ç”¨ Podman é©—è­‰ GPU çœŸçš„å¯ç”¨**

```bash
podman run --rm --device nvidia.com/gpu=all \
  nvidia/cuda:12.3.2-base-ubuntu22.04 nvidia-smi
```

è‹¥ä½ çœ‹åˆ° GPU è³‡è¨Šè¼¸å‡ºï¼ˆdriver ç‰ˆæœ¬ã€GPU åç¨±ï¼‰ï¼Œä»£è¡¨ nvidia-container-toolkit + CDI å·²å°±ç·’ã€‚

5) **æ¥è‘—å†ç”¨æœ¬å°ˆæ¡ˆ compose å•Ÿå‹•**

```bash
podman compose -f docker-compose.yaml up -d
```

##### Podman machineï¼ˆWindows / WSL2ï¼‰æ³¨æ„äº‹é …

- éœ€è¦å…ˆç¢ºä¿ **Windows ä¸»æ©Ÿ**å·²å®‰è£æ”¯æ´ WSL2 GPU çš„ NVIDIA é©…å‹•ï¼Œä¸”åœ¨ WSL2 è£¡èƒ½è·‘ `nvidia-smi`ã€‚
- `nvidia-container-toolkit` ä¹Ÿå¿…é ˆ**å®‰è£åœ¨ VM/WSL2 çš„ Linux å…§**ï¼ˆä¸æ˜¯è£åœ¨ Windows æœ¬æ©Ÿï¼‰ï¼Œä¸¦åœ¨ Linux å…§åŸ·è¡Œ `nvidia-ctk cdi generate`ã€‚
- è‹¥ä½ çš„ podman machine æ²’æœ‰ GPU passthrough èƒ½åŠ›ï¼ˆå¸¸è¦‹ï¼‰ï¼Œå³ä½¿å·¥å…·è£å¥½ä¹Ÿä¸ä¸€å®šèƒ½åœ¨ VM å…§çœ‹åˆ° GPUï¼›æ­¤æ™‚å»ºè­°æ”¹ç”¨ã€Œåœ¨ WSL2 å…§ç›´æ¥è·‘ Podmanã€æˆ–æ”¹ç”¨ Docker Desktopï¼ˆè‹¥ä½ çš„ç›®æ¨™æ˜¯ GPU å®¹å™¨ï¼‰ã€‚

##### Windowsï¼ˆWSL2ï¼‰+ podman machineï¼šåœ¨ machine å…§å®‰è£ï¼ˆä½ ç›®å‰é æœŸçš„æ–¹å¼ï¼‰

> **èƒ½ä¸èƒ½ç”¨ GPUï¼Œå–æ±ºæ–¼ GPU æ˜¯å¦èƒ½è¢« podman machine çœ‹åˆ°ã€‚**
> å¾ˆå¤š Windows çš„ podman machine æƒ…å¢ƒä¸‹ï¼ŒGPU ä¸ä¸€å®šèƒ½ passthrough åˆ° machine å…§çš„ Linux VMã€‚

1) **å…ˆç¢ºèª Windows/WSL2 çš„ GPU é©…å‹•å°±ç·’**

- ç¢ºä¿å·²å®‰è£ã€Œæ”¯æ´ WSL2ã€çš„ NVIDIA é©…å‹•ï¼ˆå»ºè­°ç”¨æœ€æ–° Studio/Game Ready Driverï¼‰ã€‚
- åœ¨ä»»ä¸€å€‹ WSL2 ç™¼è¡Œç‰ˆå…§é©—è­‰ï¼ˆå¦‚æœä½ æ²’æœ‰ WSL2 ç™¼è¡Œç‰ˆï¼Œå¯å…ˆå®‰è£ Ubuntuï¼‰ï¼š

```bash
nvidia-smi
```

è‹¥é€™ä¸€æ­¥å°±å¤±æ•—ï¼Œè«‹å…ˆæŠŠ WSL2 çš„ GPU ç’°å¢ƒä¿®å¥½ï¼›å¦å‰‡å¾Œé¢åœ¨ podman machine å…§ä¹Ÿä¸æœƒæˆåŠŸã€‚

2) **å•Ÿå‹•ä¸¦é€²å…¥ podman machine**

```powershell
podman machine start
podman machine ssh
```

3) **åœ¨ machine å…§ç¢ºèª GPU è£ç½®æ˜¯å¦å­˜åœ¨**

åœ¨ machine çš„ shell å…§åŸ·è¡Œï¼š

```bash
ls -l /dev/nvidia* 2>/dev/null || true
ls -l /dev/dxg 2>/dev/null || true
```

- è‹¥å®Œå…¨çœ‹ä¸åˆ°ä»»ä½• NVIDIA ç›¸é—œè£ç½®ï¼Œä»£è¡¨ GPU ç›®å‰æ²’æœ‰ passthrough åˆ° machineï¼šé€™ç¨®æƒ…æ³ä¸‹ï¼Œå®‰è£ toolkit ä¹Ÿç„¡æ³•è®“ GPU magically å‡ºç¾ã€‚
- è‹¥èƒ½çœ‹åˆ°è£ç½®ï¼Œæ‰å»ºè­°ç¹¼çºŒå¾€ä¸‹åšã€‚

4) **è¾¨è­˜ machine çš„ Linux é¡å‹ï¼ˆæ±ºå®šç”¨å“ªå€‹å¥—ä»¶ç®¡ç†å™¨ï¼‰**

```bash
cat /etc/os-release
command -v apt-get || true
command -v dnf || true
command -v rpm-ostree || true
```

5) **å®‰è£ nvidia-container-toolkitï¼ˆä¾ç™¼è¡Œç‰ˆï¼‰**

- **Ubuntuï¼ˆaptï¼›å« WSL2 podman machine å¸¸è¦‹æƒ…å¢ƒï¼‰**ï¼š

```bash
sudo apt-get update
sudo apt-get install -y ca-certificates curl gnupg

curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \
  | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
  | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
  | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
```

- è‹¥ machine å…§æœ‰ `dnf`ï¼ˆFedora/RHEL é¡ï¼‰ï¼š

```bash
sudo dnf install -y ca-certificates curl
sudo curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo \
  -o /etc/yum.repos.d/nvidia-container-toolkit.repo
sudo dnf install -y nvidia-container-toolkit
```

- è‹¥ machine å…§åªæœ‰ `rpm-ostree`ï¼ˆä¾‹å¦‚ CoreOS/ä¸å¯è®Šç³»çµ±ï¼‰ï¼š

  - é€™é¡ç³»çµ±ã€Œä¸ä¸€å®šã€é©åˆ/æ”¯æ´ç›´æ¥ç”¨ä¸Šè¿°æ–¹å¼å®‰è£ã€‚
  - å»ºè­°æ”¹èµ°ã€Œåœ¨ WSL2 ç™¼è¡Œç‰ˆå…§ç›´æ¥å®‰è£ Podmanã€çš„è·¯å¾‘ï¼ˆè¦‹ä¸‹æ–¹æ›¿ä»£æ–¹æ¡ˆï¼‰ã€‚

6) **ç”Ÿæˆ CDI è¨­å®š**

```bash
sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
```

ï¼ˆå»ºè­°ï¼‰è‹¥æ˜¯åœ¨ **podman machine** å…§æ“ä½œï¼Œç”Ÿæˆ CDI å¾Œå¯ä»¥é€€å‡ºä¸¦é‡å•Ÿ machineï¼Œç¢ºä¿ device è¨­å®šè¢«é‡æ–°è¼‰å…¥ï¼š

```powershell
exit
podman machine stop
podman machine start
podman machine ssh
```

7) **åœ¨ machine å…§ç”¨ Podman é©—è­‰**

```bash
podman run --rm --device nvidia.com/gpu=all \
  nvidia/cuda:12.3.2-base-ubuntu22.04 nvidia-smi
```

è‹¥ä½ åœ¨é€™ä¸€æ­¥é‡åˆ°æ¬Šé™/SELinux ç›¸é—œéŒ¯èª¤ï¼Œå¯å˜—è©¦ï¼š

- æ”¹ç”¨ rootfulï¼š`sudo podman run ...`
- æˆ–æš«æ™‚åŠ ä¸Šï¼š`--security-opt=label=disable`

8) **å†å›åˆ° Windows ç«¯å•Ÿå‹• compose**

```powershell
podman compose -f docker-compose.yaml up -d
```

##### æ›¿ä»£æ–¹æ¡ˆï¼ˆé€šå¸¸æ›´ç©©ï¼‰ï¼šåœ¨ WSL2 ç™¼è¡Œç‰ˆå…§ç›´æ¥è·‘ Podmanï¼ˆä¸ä½¿ç”¨ podman machineï¼‰

å¦‚æœä½ çš„ podman machine ç„¡æ³•çœ‹åˆ° GPUï¼Œæœ€å¸¸è¦‹çš„å¯è¡Œåšæ³•æ˜¯ï¼š

1) åœ¨ WSL2ï¼ˆUbuntuï¼‰å…§å®‰è£ Podman
2) åœ¨åŒä¸€å€‹ WSL2 å…§å®‰è£ `nvidia-container-toolkit` ä¸¦ç”Ÿæˆ CDI
3) ç›´æ¥åœ¨è©² WSL2 å…§åŸ·è¡Œ `podman run --device nvidia.com/gpu=all ...`

é€™æ¨£å®¹å™¨è·‘åœ¨ WSL2 distro å…§ï¼Œæ¯”è¼ƒå®¹æ˜“åƒåˆ° WSL2 çš„ GPU æ”¯æ´ã€‚

##### Linuxï¼ˆUbuntuï¼‰åŸç”Ÿ Podmanï¼šUbuntuï¼ˆaptï¼‰å®‰è£å‘½ä»¤

è‹¥æ˜¯åœ¨ã€ŒUbuntu Linux ä¸»æ©Ÿã€ç›´æ¥è·‘ Podmanï¼ˆä¸æ˜¯ Windows podman machineï¼‰ï¼Œå®‰è£ toolkit ä¹Ÿå¯ç”¨åŒä¸€å¥— apt æŒ‡ä»¤ï¼š

```bash
sudo apt-get update
sudo apt-get install -y ca-certificates curl gnupg

curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \
  | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \
  | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \
  | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

sudo nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml

podman run --rm --device nvidia.com/gpu=all \
  nvidia/cuda:12.3.2-base-ubuntu22.04 nvidia-smi
```

##### è‹¥ CDI å°šæœªå°±ç·’æ€éº¼è¾¦ï¼Ÿ

- è‹¥çŸ­æœŸå…§åªæƒ³å…ˆè·‘èµ·ä¾†ä¸”ä¸æƒ³è™•ç† CDIï¼Œä¹Ÿå¯ä»¥æ”¹ç”¨ Docker çš„ `--gpus all`ï¼ˆä½†é€™éœ€è¦èª¿æ•´ compose æˆ–æ”¹ç”¨ `docker run`ï¼‰ã€‚
- æœ¬å°ˆæ¡ˆ compose ç›®å‰ä½¿ç”¨ CDI å®£å‘Šï¼›è‹¥ä½ å¸Œæœ›æˆ‘å¹«ä½ è£œä¸€ä»½ã€ŒDocker é CDIã€ç‰ˆæœ¬çš„ composeï¼ˆä¾‹å¦‚ `deploy.resources.reservations.devices` æˆ– runtime è¨­å®šï¼‰ï¼Œå‘Šè¨´æˆ‘ä½ çš„ Docker ç‰ˆæœ¬èˆ‡ OSï¼Œæˆ‘å¯ä»¥ç›´æ¥åŠ æª”ã€‚

### å¥åº·æª¢æŸ¥

```bash
curl -f http://localhost:8000/api/health/
```

## ç›®éŒ„çµæ§‹

```
MutilLanguageTranslate/
â”œâ”€â”€ config/                          # é…ç½®æª”ç›®éŒ„
â”‚   â”œâ”€â”€ app_config.yaml              # æ‡‰ç”¨ç¨‹å¼é…ç½®
â”‚   â”œâ”€â”€ model_config.yaml            # æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ languages.yaml               # èªè¨€å®šç¾©
â”œâ”€â”€ logs/                            # æ—¥èªŒç›®éŒ„
â”œâ”€â”€ models/                          # æ¨¡å‹ç›®éŒ„
â”‚   â”œâ”€â”€ TAIDE-LX-7B-Chat/            # æ¨¡å‹åç¨±ä¸€ï¼ˆå»ºè­°ä½¿ç”¨ï¼Œè«‹è‡ªè¡Œä¸‹è¼‰ï¼‰
â”‚   â””â”€â”€ Llama-3.1-TAIDE-LX-8B-Chat/  # æ¨¡å‹åç¨±äºŒï¼ˆå»ºè­°ä½¿ç”¨ï¼Œè«‹è‡ªè¡Œä¸‹è¼‰ï¼‰
â”œâ”€â”€ translation_project/             # Django å°ˆæ¡ˆ
â”‚   â”œâ”€â”€ translation_project/         # å°ˆæ¡ˆè¨­å®š
â”‚   â”‚   â”œâ”€â”€ settings.py
â”‚   â”‚   â”œâ”€â”€ urls.py
â”‚   â”‚   â””â”€â”€ asgi.py
â”‚   â””â”€â”€ translator/                  # ç¿»è­¯æ‡‰ç”¨ç¨‹å¼
â”‚       â”œâ”€â”€ api/                     # REST API
â”‚       â”œâ”€â”€ services/                # æœå‹™å±¤
â”‚       â”œâ”€â”€ templates/               # å‰ç«¯æ¨¡æ¿
â”‚       â”œâ”€â”€ static/                  # éœæ…‹è³‡æº
â”‚       â””â”€â”€ utils/                   # å·¥å…·å‡½æ•¸
â”œâ”€â”€ specs/                           # è¦æ ¼æ–‡ä»¶
â”œâ”€â”€ tests/                           # æ¸¬è©¦
â”œâ”€â”€ Containerfile                    # å®¹å™¨å»ºç½®æª”
â”œâ”€â”€ docker-compose.yaml              # Docker Compose
â”œâ”€â”€ requirements.txt                 # Python ç›¸ä¾å¥—ä»¶
â””â”€â”€ README.md                        # æœ¬æ–‡ä»¶
```

## API æ–‡ä»¶

### ç¿»è­¯ API

#### POST /api/v1/translate/

åŸ·è¡Œæ–‡å­—ç¿»è­¯ã€‚

**è«‹æ±‚**
```json
{
  "text": "è¦ç¿»è­¯çš„æ–‡å­—",
  "source_language": "auto",
  "target_language": "en",
  "quality": "standard"
}
```

**å›æ‡‰**
```json
{
  "request_id": "uuid",
  "status": "completed",
  "translated_text": "Translated text",
  "processing_time_ms": 1234.56,
  "detected_language": "zh-TW"
}
```

### å¥åº·æª¢æŸ¥ API

#### GET /api/health/

å›å‚³ç³»çµ±å¥åº·ç‹€æ…‹ã€‚

#### GET /api/ready/

å°±ç·’æ¢é‡ï¼Œæª¢æŸ¥æœå‹™æ˜¯å¦æº–å‚™å¥½æ¥æ”¶æµé‡ã€‚

#### GET /api/live/

å­˜æ´»æ¢é‡ï¼Œæª¢æŸ¥æœå‹™æ˜¯å¦å­˜æ´»ã€‚

### èªè¨€ API

#### GET /api/v1/languages/

å–å¾—æ”¯æ´çš„èªè¨€æ¸…å–®ã€‚

### ç®¡ç† API

#### POST /api/v1/admin/model/test/

æ¸¬è©¦è¼‰å…¥å°å‹æ¨¡å‹ï¼ˆç”¨æ–¼é©—è­‰ç’°å¢ƒèˆ‡é‡åŒ–è¨­å®šï¼‰ã€‚

**è«‹æ±‚**
```json
{
  "model_name": "gpt2"
}
```

**å›æ‡‰**
```json
{
  "success": true,
  "message": "å°æ¨¡å‹è¼‰å…¥èˆ‡æ¨è«–æˆåŠŸ (gpt2)",
  "model_info": {
    "model_name": "gpt2",
    "generated": "Hello world, I'm not sure what to say",
    "cuda_available": true,
    "cuda_device_count": 1
  }
}
```

## é…ç½®èªªæ˜

### app_config.yaml

```yaml
server:
  host: "0.0.0.0"
  port: 8000
  workers: 1

translation:
  default_source_language: "auto"
  default_target_language: "en"
  max_text_length: 5000
  timeout_seconds: 120

security:
  admin_ip_whitelist:
    - "127.0.0.1"
    - "::1"
```

### model_config.yaml

```yaml
model:
  name: "taide/TAIDE-LX-7B"
  cache_dir: "./models"
  device: "auto"  # auto, cuda, cpu
  torch_dtype: "auto"

inference:
  max_new_tokens: 2048
  temperature: 0.7
  top_p: 0.9
```

## ç•Œé¢ç¯„ä¾‹

![](./Docs/images/Interfase.png)
![](./Docs/images/History.png)
![](./Docs/images/Setting.png)

## æˆæ¬Š

MIT License

## è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼

## è¯çµ¡

å¦‚æœ‰å•é¡Œï¼Œè«‹é€é GitHub Issues è¯çµ¡æˆ‘å€‘ã€‚
